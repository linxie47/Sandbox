From 4b8967e8a428008d64ff6a206bb2e76c8807ce81 Mon Sep 17 00:00:00 2001
From: Lin Xie <lin.xie@intel.com>
Date: Fri, 10 May 2019 15:46:56 +0800
Subject: [PATCH] Face reidentification refine

* add filter metaconvert for convert tensor
* add filter identify to do reidentification
* clean up filter classify, moved face identify
  special processings to filter identify
* borrow gallery_generator.py from gst plugin

Change-Id: I0e0c6889eeb51a366ebe244708667af7f3386302
---
 configure                                          |   3 +
 libavfilter/Makefile                               |   2 +
 libavfilter/allfilters.c                           |   2 +
 libavfilter/inference.h                            |   5 +-
 libavfilter/vf_inference_classify.c                | 335 +++------------------
 libavfilter/vf_inference_identify.c                | 328 ++++++++++++++++++++
 libavfilter/vf_inference_metaconvert.c             | 190 ++++++++++++
 .../shell/face_detection_and_reidentification.sh   | 131 ++++++++
 samples/shell/reidentification/README.md           |  38 +++
 .../Basketball_Player_0_frame_0_idx_0.tensor       | Bin 0 -> 1024 bytes
 .../Basketball_Player_0_frame_0_idx_1.tensor       | Bin 0 -> 1024 bytes
 ...erson-One_curly_hair_boy_1_frame_0_idx_0.tensor | Bin 0 -> 1024 bytes
 ...erson-One_pink_hair_girl_7_frame_0_idx_0.tensor | Bin 0 -> 1024 bytes
 ...son-One_yellow_hair_girl_5_frame_0_idx_0.tensor | Bin 0 -> 1024 bytes
 .../features/Person-Two_man_2_frame_0_idx_0.tensor | Bin 0 -> 1024 bytes
 .../features/TomHanks_8_frame_0_idx_0.tensor       | Bin 0 -> 1024 bytes
 ...pshot_COR_VAN_DER_KLAAUW_3_frame_0_idx_0.tensor | Bin 0 -> 1024 bytes
 .../snapshot_DAVID_HEMBROW_6_frame_0_idx_0.tensor  | Bin 0 -> 1024 bytes
 ...hot_PROF_GREG_J_ASHWORTH_4_frame_0_idx_0.tensor | Bin 0 -> 1024 bytes
 .../shell/reidentification/gallery/gallery.json    |   1 +
 .../shell/reidentification/gallery_generator.py    | 143 +++++++++
 21 files changed, 883 insertions(+), 295 deletions(-)
 create mode 100644 libavfilter/vf_inference_identify.c
 create mode 100644 libavfilter/vf_inference_metaconvert.c
 create mode 100755 samples/shell/face_detection_and_reidentification.sh
 create mode 100644 samples/shell/reidentification/README.md
 create mode 100644 samples/shell/reidentification/gallery/features/Basketball_Player_0_frame_0_idx_0.tensor
 create mode 100644 samples/shell/reidentification/gallery/features/Basketball_Player_0_frame_0_idx_1.tensor
 create mode 100644 samples/shell/reidentification/gallery/features/Person-One_curly_hair_boy_1_frame_0_idx_0.tensor
 create mode 100644 samples/shell/reidentification/gallery/features/Person-One_pink_hair_girl_7_frame_0_idx_0.tensor
 create mode 100644 samples/shell/reidentification/gallery/features/Person-One_yellow_hair_girl_5_frame_0_idx_0.tensor
 create mode 100644 samples/shell/reidentification/gallery/features/Person-Two_man_2_frame_0_idx_0.tensor
 create mode 100644 samples/shell/reidentification/gallery/features/TomHanks_8_frame_0_idx_0.tensor
 create mode 100644 samples/shell/reidentification/gallery/features/snapshot_COR_VAN_DER_KLAAUW_3_frame_0_idx_0.tensor
 create mode 100644 samples/shell/reidentification/gallery/features/snapshot_DAVID_HEMBROW_6_frame_0_idx_0.tensor
 create mode 100644 samples/shell/reidentification/gallery/features/snapshot_PROF_GREG_J_ASHWORTH_4_frame_0_idx_0.tensor
 create mode 100644 samples/shell/reidentification/gallery/gallery.json
 create mode 100644 samples/shell/reidentification/gallery_generator.py

diff --git a/configure b/configure
index b8f9c4a..dcaaf95 100755
--- a/configure
+++ b/configure
@@ -3416,6 +3416,9 @@ inference_classify_filter_deps="libinference_engine libcjson"
 inference_classify_filter_select="dnn"
 inference_detect_filter_deps="libinference_engine libcjson"
 inference_detect_filter_select="dnn"
+inference_identify_filter_deps="libinference_engine libcjson"
+inference_identify_filter_select="dnn"
+inference_metaconvert_filter_deps="libinference_engine libcjson"
 interlace_filter_deps="gpl"
 kerndeint_filter_deps="gpl"
 ladspa_filter_deps="ladspa libdl"
diff --git a/libavfilter/Makefile b/libavfilter/Makefile
index d9e0602..11f0fb4 100644
--- a/libavfilter/Makefile
+++ b/libavfilter/Makefile
@@ -260,6 +260,8 @@ OBJS-$(CONFIG_IDET_FILTER)                   += vf_idet.o
 OBJS-$(CONFIG_IL_FILTER)                     += vf_il.o
 OBJS-$(CONFIG_INFERENCE_CLASSIFY_FILTER)     += vf_inference_classify.o
 OBJS-$(CONFIG_INFERENCE_DETECT_FILTER)       += vf_inference_detect.o
+OBJS-$(CONFIG_INFERENCE_IDENTIFY_FILTER)     += vf_inference_identify.o
+OBJS-$(CONFIG_INFERENCE_METACONVERT_FILTER)  += vf_inference_metaconvert.o
 OBJS-$(CONFIG_INFLATE_FILTER)                += vf_neighbor.o
 OBJS-$(CONFIG_INTERLACE_FILTER)              += vf_tinterlace.o
 OBJS-$(CONFIG_INTERLEAVE_FILTER)             += f_interleave.o
diff --git a/libavfilter/allfilters.c b/libavfilter/allfilters.c
index 158c75b..4588c2b 100644
--- a/libavfilter/allfilters.c
+++ b/libavfilter/allfilters.c
@@ -246,6 +246,8 @@ extern AVFilter ff_vf_idet;
 extern AVFilter ff_vf_il;
 extern AVFilter ff_vf_inference_classify;
 extern AVFilter ff_vf_inference_detect;
+extern AVFilter ff_vf_inference_identify;
+extern AVFilter ff_vf_inference_metaconvert;
 extern AVFilter ff_vf_inflate;
 extern AVFilter ff_vf_interlace;
 extern AVFilter ff_vf_interleave;
diff --git a/libavfilter/inference.h b/libavfilter/inference.h
index 0512403..1d7971e 100644
--- a/libavfilter/inference.h
+++ b/libavfilter/inference.h
@@ -193,10 +193,13 @@ typedef struct InferDetectionMeta {
 typedef struct InferClassification {
     int     detect_id;        ///< detected bbox index
     char   *name;             ///< class name, e.g. emotion, age
+    char   *layer_name;       ///< output layer name
+    char   *model;            ///< model name
     int     label_id;         ///< label index in labels
     float   confidence;
     float   value;
-    AVBufferRef *label_buf;   ///< label ref buf from label file
+    AVBufferRef *label_buf;   ///< label buffer
+    AVBufferRef *tensor_buf;  ///< output tensor buffer
 } InferClassification;
 
 /* dynamic classifications array */
diff --git a/libavfilter/vf_inference_classify.c b/libavfilter/vf_inference_classify.c
index 2367a8c..b36a7ca 100644
--- a/libavfilter/vf_inference_classify.c
+++ b/libavfilter/vf_inference_classify.c
@@ -40,13 +40,7 @@
 #define OFFSET(x) offsetof(InferenceClassifyContext, x)
 #define FLAGS (AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_FILTERING_PARAM)
 
-#define PI 3.1415926
 #define MAX_MODEL_NUM 8
-#define FACE_FEATURE_VECTOR_LEN 256
-
-typedef int (*ClassifyInit)(AVFilterContext *ctx, size_t index);
-
-typedef int (*ClassifyUnInit)(AVFilterContext *ctx, size_t index);
 
 typedef int (*ClassifyProcess)(AVFilterContext*, int, int, int,
                                InferTensorMeta*, InferClassificationMeta*);
@@ -56,15 +50,10 @@ typedef struct InferenceClassifyContext {
 
     InferenceBaseContext *infer_bases[MAX_MODEL_NUM];
 
-    char  *labels;
-    char  *names;
-
     char  *model_file;
     char  *model_proc;
     char  *vpp_format;
-    char  *feature_file;    ///< binary feature file for face identification
-    int    feature_num;     ///< identification face feature number
-    double feature_angle;   ///< face identification threshold angle value
+
     int    loaded_num;
     int    backend_type;
     int    device_type;
@@ -73,12 +62,6 @@ typedef struct InferenceClassifyContext {
     int    frame_number;
     int    every_nth_frame;
 
-    void           *priv[MAX_MODEL_NUM];
-    char           *name_array[MAX_MODEL_NUM];
-    AVBufferRef    *label_bufs[MAX_MODEL_NUM];
-
-    ClassifyInit    init[MAX_MODEL_NUM];
-    ClassifyUnInit  uninit[MAX_MODEL_NUM];
     ClassifyProcess post_process[MAX_MODEL_NUM];
 
     void *proc_config[MAX_MODEL_NUM];
@@ -86,23 +69,6 @@ typedef struct InferenceClassifyContext {
     ModelOutputPostproc model_postproc[MAX_MODEL_NUM];
 } InferenceClassifyContext;
 
-typedef struct FaceIdentifyContext {
-    size_t   vector_num;
-    double  *norm_std;
-    float  **feature_vecs;
-} FaceIdentifyContext;
-
-static void infer_labels_buffer_free(void *opaque, uint8_t *data)
-{
-    int i;
-    LabelsArray *labels = (LabelsArray *)data;
-
-    for (i = 0; i < labels->num; i++)
-        av_freep(&labels->label[i]);
-
-    av_free(data);
-}
-
 static void infer_classify_metadata_buffer_free(void *opaque, uint8_t *data)
 {
     int i;
@@ -113,6 +79,7 @@ static void infer_classify_metadata_buffer_free(void *opaque, uint8_t *data)
         for (i = 0; i < classes->num; i++) {
             InferClassification *c = classes->classifications[i];
             av_buffer_unref(&c->label_buf);
+            av_buffer_unref(&c->tensor_buf);
             av_freep(&c);
         }
         av_freep(&classes);
@@ -265,6 +232,40 @@ static int tensor_to_text(AVFilterContext *ctx,
     return 0;
 }
 
+static int default_postprocess(AVFilterContext *ctx,
+                               int detect_id,
+                               int result_id,
+                               int model_id,
+                               InferTensorMeta *meta,
+                               InferClassificationMeta *c_meta)
+{
+    InferenceClassifyContext *s = ctx->priv;
+    InferenceBaseContext *base  = s->infer_bases[model_id];
+    DNNModelInfo *info = ff_inference_base_get_output_info(base);
+    InferClassification *classify;
+
+    if (!meta->data) return -1;
+
+    classify = av_mallocz(sizeof(*classify));
+    if (!classify)
+        return AVERROR(ENOMEM);
+
+    classify->detect_id  = detect_id;
+    classify->layer_name = info->layer_name[result_id];
+    classify->model      = s->model_file;
+
+    classify->tensor_buf = av_buffer_alloc(meta->total_bytes);
+    if (!classify->tensor_buf)
+        return AVERROR(ENOMEM);
+    if (meta->total_bytes > 0)
+        memcpy(classify->tensor_buf->data, meta->data, meta->total_bytes);
+
+    av_dynarray_add(&c_meta->c_array->classifications, &c_meta->c_array->num, classify);
+
+    av_log(ctx, AV_LOG_DEBUG, "default output[%s] size: %zu\n", classify->layer_name, meta->total_bytes);
+    return 0;
+}
+
 static int commmon_postprocess(AVFilterContext *ctx,
                                int detect_id,
                                int result_id,
@@ -299,7 +300,7 @@ static int commmon_postprocess(AVFilterContext *ctx,
     proc = &s->model_postproc[model_id].procs[proc_id];
 
     if (proc->converter == NULL)
-        return 0;
+        return default_postprocess(ctx, detect_id, result_id, model_id, meta, c_meta);
 
     if (!strcmp(proc->converter, "attributes"))
         return attributes_to_text(ctx, detect_id, proc, meta, c_meta);
@@ -310,193 +311,6 @@ static int commmon_postprocess(AVFilterContext *ctx,
     return 0;
 }
 
-static int face_identify_init(AVFilterContext *ctx, size_t index)
-{
-    FaceIdentifyContext *identify_ctx;
-    InferenceClassifyContext *s = ctx->priv;
-
-    int i, ret, feature_size, expected_size;
-    size_t vec_size_in_bytes = sizeof(float) * FACE_FEATURE_VECTOR_LEN;
-
-    FILE *fp = fopen(s->feature_file, "rb");
-    if (!fp) {
-        av_log(ctx, AV_LOG_ERROR, "Could not open feature file:%s\n", s->feature_file);
-        return AVERROR(EIO);
-    }
-
-    av_assert0(index < MAX_MODEL_NUM);
-
-    feature_size = ff_get_file_size(fp);
-
-    if (feature_size == -1) {
-        fclose(fp);
-        av_log(ctx, AV_LOG_ERROR, "Couldn't get size of feature file.\n");
-        return AVERROR(EINVAL);
-    } else if (feature_size % FACE_FEATURE_VECTOR_LEN) {
-        fclose(fp);
-        av_log(ctx, AV_LOG_ERROR, "Feature data must align to %d.\n", FACE_FEATURE_VECTOR_LEN);
-        return AVERROR(EINVAL);
-    }
-
-    if (s->feature_num > 0) {
-        expected_size = s->feature_num * vec_size_in_bytes;
-        if (expected_size != feature_size) {
-            fclose(fp);
-            av_log(ctx, AV_LOG_ERROR, "Unexpected feature file size.\n");
-            return AVERROR(EINVAL);
-        }
-    } else {
-        s->feature_num = feature_size / vec_size_in_bytes;
-    }
-
-    identify_ctx = av_mallocz(sizeof(*identify_ctx));
-    if (!identify_ctx) {
-        ret = AVERROR(ENOMEM);
-        goto fail;
-    }
-
-    identify_ctx->vector_num = s->feature_num;
-
-    identify_ctx->feature_vecs = av_mallocz(sizeof(float *) * identify_ctx->vector_num);
-    if (!identify_ctx->feature_vecs) {
-        ret = AVERROR(ENOMEM);
-        goto fail;
-    }
-
-    rewind(fp);
-
-    for (i = 0; i <identify_ctx->vector_num; i++) {
-        identify_ctx->feature_vecs[i] = av_malloc(vec_size_in_bytes);
-        if (!identify_ctx->feature_vecs[i]) {
-            ret = AVERROR(ENOMEM);
-            goto fail;
-        }
-        if (fread(identify_ctx->feature_vecs[i], vec_size_in_bytes, 1, fp) != 1) {
-            ret = AVERROR(EINVAL);
-            goto fail;
-        }
-    }
-
-    identify_ctx->norm_std = av_mallocz(sizeof(double) * identify_ctx->vector_num);
-    if (!identify_ctx->norm_std) {
-        ret = AVERROR(ENOMEM);
-        goto fail;
-    }
-
-    for (i = 0; i < identify_ctx->vector_num; i++)
-        identify_ctx->norm_std[i] = av_norm(identify_ctx->feature_vecs[i],
-                                            FACE_FEATURE_VECTOR_LEN);
-
-    s->priv[index] = identify_ctx;
-    fclose(fp);
-    return 0;
-fail:
-    fclose(fp);
-
-    if (identify_ctx) {
-        if (identify_ctx->feature_vecs) {
-            for (i = 0; i <identify_ctx->vector_num; i++) {
-                if (identify_ctx->feature_vecs[i])
-                    av_free(identify_ctx->feature_vecs[i]);
-            }
-            av_free(identify_ctx->feature_vecs);
-        }
-        av_free(identify_ctx);
-    }
-    return ret;
-}
-
-static int face_identify_uninit(AVFilterContext *ctx, size_t index)
-{
-    int i;
-    InferenceClassifyContext *s = ctx->priv;
-    FaceIdentifyContext *identify_ctx = s->priv[index];
-
-    if (!identify_ctx) {
-        av_log(ctx, AV_LOG_WARNING, "Empty face identify ctx.\n");
-        return 0;
-    }
-
-    if (identify_ctx->feature_vecs) {
-        for (i = 0; i < identify_ctx->vector_num; i++)
-            av_free(identify_ctx->feature_vecs[i]);
-        av_free(identify_ctx->feature_vecs);
-    }
-
-    if (identify_ctx->norm_std)
-        av_free(identify_ctx->norm_std);
-
-    av_free(identify_ctx);
-    s->priv[index] = NULL;
-
-    return 0;
-}
-
-static av_cold void dump_face_id(AVFilterContext *ctx, int label_id,
-                                 float conf, AVBufferRef *label_buf)
-{
-    LabelsArray *array = (LabelsArray *)label_buf->data;
-
-    av_log(ctx, AV_LOG_DEBUG,"CLASSIFY META - Face_id:%d Name:%s Conf:%1.2f\n",
-           label_id, array->label[label_id], conf);
-}
-
-static int face_identify_result_process(AVFilterContext *ctx,
-                                        int detect_id,
-                                        int result_id,
-                                        int model_index,
-                                        InferTensorMeta *meta,
-                                        InferClassificationMeta *c_meta)
-{
-    int i, label_id = 0;
-    InferClassification *classify;
-    double dot_product, norm_feature, confidence, *angles;
-    InferenceClassifyContext *s = ctx->priv;
-    FaceIdentifyContext      *f = s->priv[model_index];
-    double            min_angle = 180.0f;
-    float       *feature_vector = meta->data;
-
-    angles = av_malloc(sizeof(double) * f->vector_num);
-    if (!angles)
-        return AVERROR(ENOMEM);
-
-    norm_feature = av_norm(feature_vector, FACE_FEATURE_VECTOR_LEN);
-
-    for (i = 0; i < f->vector_num; i++) {
-        dot_product = av_dot(feature_vector,
-                             f->feature_vecs[i],
-                             FACE_FEATURE_VECTOR_LEN);
-
-        angles[i] = acos((dot_product - 0.0001f) /
-                         (f->norm_std[i] * norm_feature)) /
-                    PI * 180.0;
-        if (angles[i] < s->feature_angle && angles[i] < min_angle) {
-            label_id  = i;
-            min_angle = angles[i];
-        }
-    }
-
-    confidence = (90.0f - min_angle) / 90.0f;
-
-    av_free(angles);
-
-    classify = av_mallocz(sizeof(*classify));
-    if (!classify)
-        return AVERROR(ENOMEM);
-
-    classify->detect_id  = detect_id;
-    classify->name       = s->name_array[model_index];
-    classify->label_id   = label_id;
-    classify->confidence = (float)confidence;
-    classify->label_buf  = av_buffer_ref(s->label_bufs[model_index]);
-
-    dump_face_id(ctx, label_id, confidence, s->label_bufs[model_index]);
-
-    av_dynarray_add(&c_meta->c_array->classifications, &c_meta->c_array->num, classify);
-
-    return 0;
-}
-
 static int query_formats(AVFilterContext *context)
 {
     AVFilterFormats *formats_list;
@@ -520,11 +334,9 @@ static av_cold int classify_init(AVFilterContext *ctx)
 {
     InferenceClassifyContext *s = ctx->priv;
     int i, ret;
-    int model_num = 0, model_proc_num = 0, label_num = 0, name_num = 0;
+    int model_num = 0, model_proc_num = 0;
     const int max_num = MAX_MODEL_NUM;
-    char  *names[MAX_MODEL_NUM] = { };
     char *models[MAX_MODEL_NUM] = { };
-    char *labels[MAX_MODEL_NUM] = { };
     char *models_proc[MAX_MODEL_NUM] = { };
     InferenceParam p = {};
 
@@ -534,21 +346,10 @@ static av_cold int classify_init(AVFilterContext *ctx)
     for (i = 0; i < model_num; i++)
         av_log(ctx, AV_LOG_INFO, "model[%d]:%s\n", i, models[i]);
 
-    av_split(s->labels, "&", labels, &label_num, max_num);
-    for (i = 0; i < label_num; i++)
-        av_log(ctx, AV_LOG_INFO, "label[%d]:%s\n", i, labels[i]);
-
-    av_split(s->names, "&", names, &name_num, max_num);
-    for (i = 0; i < name_num; i++)
-        av_log(ctx, AV_LOG_INFO, "name[%d]:%s\n", i, names[i]);
-
     av_split(s->model_proc, "&", models_proc, &model_proc_num, max_num);
     for (i = 0; i < model_proc_num; i++)
         av_log(ctx, AV_LOG_INFO, "proc[%d]:%s\n", i, models_proc[i]);
 
-    // TODO: uncomment this after face reidentify use proc file
-    // av_assert0(model_proc_num == model_num);
-
     av_assert0(s->backend_type == DNN_INTEL_IE);
 
     p.backend_type    = s->backend_type;
@@ -600,53 +401,11 @@ static av_cold int classify_init(AVFilterContext *ctx)
     }
     s->loaded_num = model_num;
 
-    for (i = 0; i < label_num; i++) {
-        int n, labels_num;
-        AVBufferRef *ref    = NULL;
-        LabelsArray *larray = NULL;
-        char buffer[4096]   = { };
-        char *_labels[100]  = { };
-
-        FILE *fp = fopen(labels[i], "rb");
-        if (!fp) {
-            av_log(ctx, AV_LOG_ERROR, "Could not open file:%s\n", labels[i]);
-            ret = AVERROR(EIO);
-            goto fail;
-        }
-
-        n = fread(buffer, sizeof(buffer), 1, fp);
-        fclose(fp);
-
-        av_split(buffer, ",", _labels, &labels_num, 100);
-
-        larray = av_mallocz(sizeof(*larray));
-        if (!larray) {
-            ret = AVERROR(ENOMEM);
-            goto fail;
-        }
-
-        for (n = 0; n < labels_num; n++) {
-            char *l = av_strdup(_labels[n]);
-            av_dynarray_add(&larray->label, &larray->num, l);
-        }
-
-        ref = av_buffer_create((uint8_t *)larray, sizeof(*larray),
-                               &infer_labels_buffer_free, NULL, 0);
-        s->label_bufs[i] = ref;
-    }
-
     for (i = 0; i < model_num; i++) {
-        s->name_array[i] = names[i];
-        if (names[i] && strstr(names[i], "face")) {
-            s->init[i]         = &face_identify_init;
-            s->uninit[i]       = &face_identify_uninit;
-            s->post_process[i] = &face_identify_result_process;
-        } else {
+        if (!models_proc[i])
+            s->post_process[i] = &default_postprocess;
+        else
             s->post_process[i] = &commmon_postprocess;
-        }
-
-        if (s->init[i] && s->init[i](ctx, i) < 0)
-            goto fail;
     }
 
     return 0;
@@ -654,8 +413,6 @@ static av_cold int classify_init(AVFilterContext *ctx)
 fail:
     for (i = 0; i < model_num; i++) {
         ff_inference_base_free(&s->infer_bases[i]);
-        if (s->label_bufs[i])
-            av_buffer_unref(&s->label_bufs[i]);
     }
 
     return ret;
@@ -667,12 +424,7 @@ static av_cold void classify_uninit(AVFilterContext *ctx)
     InferenceClassifyContext *s = ctx->priv;
 
     for (i = 0; i < s->loaded_num; i++) {
-        if (s->uninit[i]) s->uninit[i](ctx, i);
-
         ff_inference_base_free(&s->infer_bases[i]);
-
-        av_buffer_unref(&s->label_bufs[i]);
-
         ff_release_model_proc(s->proc_config[i], &s->model_preproc[i], &s->model_postproc[i]);
     }
 }
@@ -926,15 +678,10 @@ static const AVOption inference_classify_options[] = {
     { "dnn_backend",    "DNN backend for model execution", OFFSET(backend_type),    AV_OPT_TYPE_FLAGS,  { .i64 = DNN_INTEL_IE },          0, 2,    FLAGS, "engine" },
     { "model",          "path to model files for network", OFFSET(model_file),      AV_OPT_TYPE_STRING, { .str = NULL},                   0, 0,    FLAGS },
     { "model_proc",     "model preproc and postproc",      OFFSET(model_proc),      AV_OPT_TYPE_STRING, { .str = NULL},                   0, 0,    FLAGS },
-    { "label",          "labels for classify",             OFFSET(labels),          AV_OPT_TYPE_STRING, { .str = NULL},                   0, 0,    FLAGS },
-    { "name",           "classify type names",             OFFSET(names),           AV_OPT_TYPE_STRING, { .str = NULL},                   0, 0,    FLAGS },
     { "vpp_format",     "specify vpp output format",       OFFSET(vpp_format),      AV_OPT_TYPE_STRING, { .str = NULL},                   0, 0,    FLAGS },
     { "device",         "running on device type",          OFFSET(device_type),     AV_OPT_TYPE_FLAGS,  { .i64 = DNN_TARGET_DEVICE_CPU }, 0, 12,   FLAGS },
     { "interval",       "do infer every Nth frame",        OFFSET(every_nth_frame), AV_OPT_TYPE_INT,    { .i64 = 1 },                     1, 1024, FLAGS },
     { "batch_size",     "batch size per infer",            OFFSET(batch_size),      AV_OPT_TYPE_INT,    { .i64 = 1 },                     1, 1024, FLAGS },
-    { "feature_file",   "registered face feature data",    OFFSET(feature_file),    AV_OPT_TYPE_STRING, { .str = NULL},                   0,    0, FLAGS, "face_identify" },
-    { "feature_num",    "registered face number",          OFFSET(feature_num),     AV_OPT_TYPE_INT,    { .i64 = 0},                      0, 1024, FLAGS, "face_identify" },
-    { "identify_angle", "face identify threshold angle",   OFFSET(feature_angle),   AV_OPT_TYPE_DOUBLE, { .dbl = 70},                     0, 90,   FLAGS, "face_identify" },
     { NULL }
 };
 
diff --git a/libavfilter/vf_inference_identify.c b/libavfilter/vf_inference_identify.c
new file mode 100644
index 0000000..5e676dd
--- /dev/null
+++ b/libavfilter/vf_inference_identify.c
@@ -0,0 +1,328 @@
+/*
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * dnn inference identify filter
+ */
+#include "libavutil/opt.h"
+#include "libavutil/mem.h"
+#include "libavutil/eval.h"
+#include "libavutil/avassert.h"
+#include "libavutil/avstring.h"
+#include "libavutil/pixdesc.h"
+#include "libavformat/avformat.h"
+
+#include "formats.h"
+#include "internal.h"
+#include "avfilter.h"
+
+#include "inference.h"
+
+#include <cjson/cJSON.h>
+
+#define OFFSET(x) offsetof(InferenceIdentifyContext, x)
+#define FLAGS (AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_FILTERING_PARAM)
+
+#define PI 3.1415926
+#define FACE_FEATURE_VECTOR_LEN 256
+
+typedef struct FeatureLabelPair {
+    float *feature;
+    size_t label_id;
+} FeatureLabelPair;
+
+typedef struct InferenceIdentifyContext {
+    const AVClass *class;
+
+    char   *gallery;      ///<< gallery for identify features
+    double *norm_std;
+
+    AVBufferRef *labels;
+    FeatureLabelPair **features;
+    int features_num;
+} InferenceIdentifyContext;
+
+static const char *get_filename_ext(const char *filename) {
+    const char *dot = strrchr(filename, '.');
+    if (!dot || dot == filename)
+        return NULL;
+
+    return dot + 1;
+}
+
+const char *gallery_file_suffix = "json";
+
+static void infer_labels_buffer_free(void *opaque, uint8_t *data)
+{
+    int i;
+    LabelsArray *labels = (LabelsArray *)data;
+
+    for (i = 0; i < labels->num; i++)
+        av_freep(&labels->label[i]);
+
+    av_free(data);
+}
+
+static int query_formats(AVFilterContext *context)
+{
+    AVFilterFormats *formats_list;
+    const enum AVPixelFormat pixel_formats[] = {
+        AV_PIX_FMT_YUV420P,  AV_PIX_FMT_YUV422P,  AV_PIX_FMT_YUV444P,
+        AV_PIX_FMT_YUVJ420P, AV_PIX_FMT_YUVJ422P, AV_PIX_FMT_YUVJ444P,
+        AV_PIX_FMT_YUV410P,  AV_PIX_FMT_YUV411P,  AV_PIX_FMT_GRAY8,
+        AV_PIX_FMT_BGR24,    AV_PIX_FMT_BGRA,     AV_PIX_FMT_VAAPI,
+        AV_PIX_FMT_NONE};
+
+    formats_list = ff_make_format_list(pixel_formats);
+    if (!formats_list) {
+        av_log(context, AV_LOG_ERROR, "Could not create formats list\n");
+        return AVERROR(ENOMEM);
+    }
+
+    return ff_set_common_formats(context, formats_list);
+}
+
+static av_cold int identify_init(AVFilterContext *ctx)
+{
+    size_t i, index = 1;
+    char *dup, *unknown;
+    const char *dirname;
+    cJSON *entry, *item;
+    LabelsArray *larray = NULL;
+    AVBufferRef *ref    = NULL;
+    InferenceIdentifyContext *s = ctx->priv;
+    size_t vec_size_in_bytes = sizeof(float) * FACE_FEATURE_VECTOR_LEN;
+
+    av_assert0(s->gallery);
+
+    if (strcmp(get_filename_ext(s->gallery), gallery_file_suffix)) {
+        av_log(ctx, AV_LOG_ERROR, "Face gallery '%s' is not a json file\n", s->gallery);
+        return AVERROR(EINVAL);
+    }
+
+    entry = ff_read_model_proc(s->gallery);
+    if (!entry) {
+        av_log(ctx, AV_LOG_ERROR, "Could not open gallery file:%s\n", s->gallery);
+        return AVERROR(EIO);
+    }
+
+    dup = av_strdup(s->gallery);
+    dirname = av_dirname(dup);
+
+    larray = av_mallocz(sizeof(*larray));
+    if (!larray)
+        return AVERROR(ENOMEM);
+
+    // label id 0 reserved for unknown person
+    unknown = av_strdup("Unknown_Person");
+    av_dynarray_add(&larray->label, &larray->num, unknown);
+
+    cJSON_ArrayForEach(item, entry)
+    {
+        char *l = av_strdup(item->string);
+        cJSON *features, *feature;
+
+        av_dynarray_add(&larray->label, &larray->num, l);
+
+        features = cJSON_GetObjectItem(item, "features");
+
+        cJSON_ArrayForEach(feature, features)
+        {
+            FILE *vec_fp;
+            FeatureLabelPair *pair;
+            char path[4096];
+
+            memset(path, 0, sizeof(path));
+
+            if (!cJSON_IsString(feature) || !feature->valuestring)
+                continue;
+
+            strncpy(path, dirname, strlen(dirname));
+            strncat(path, "/", 1);
+            strncat(path, feature->valuestring, strlen(feature->valuestring));
+
+            vec_fp = fopen(path, "rb");
+            if (!vec_fp) {
+                av_log(ctx, AV_LOG_ERROR, "Could not open feature file:%s\n", path);
+                continue;
+            }
+
+            pair = av_mallocz(sizeof(FeatureLabelPair));
+            if (!pair)
+                return AVERROR(ENOMEM);
+
+            pair->feature = av_malloc(vec_size_in_bytes);
+            if (!pair->feature)
+                return AVERROR(ENOMEM);
+
+            if (fread(pair->feature, vec_size_in_bytes, 1, vec_fp) != 1) {
+                av_log(ctx, AV_LOG_ERROR, "Feature vector size mismatch:%s\n", path);
+                fclose(vec_fp);
+                return AVERROR(EINVAL);
+            }
+
+            fclose(vec_fp);
+
+            pair->label_id = index;
+            av_dynarray_add(&s->features, &s->features_num, pair);
+        }
+        index++;
+    }
+
+    s->norm_std = av_mallocz(sizeof(double) * s->features_num);
+    if (!s->norm_std)
+        return AVERROR(ENOMEM);
+
+    for (i = 0; i < s->features_num; i++)
+        s->norm_std[i] = av_norm(s->features[i]->feature, FACE_FEATURE_VECTOR_LEN);
+
+    ref = av_buffer_create((uint8_t *)larray, sizeof(*larray),
+            &infer_labels_buffer_free, NULL, 0);
+
+    s->labels = ref;
+    av_free(dup);
+
+    return 0;
+}
+
+static av_cold void identify_uninit(AVFilterContext *ctx)
+{
+    int i;
+    InferenceIdentifyContext *s = ctx->priv;
+
+    av_buffer_unref(&s->labels);
+
+    for (i = 0; i < s->features_num; i++) {
+        av_freep(&s->features[i]->feature);
+        av_freep(&s->features[i]);
+    }
+    if (s->norm_std)
+        av_free(s->norm_std);
+}
+
+static av_cold void dump_face_id(AVFilterContext *ctx, int label_id,
+                                 float conf, AVBufferRef *label_buf)
+{
+    LabelsArray *array = (LabelsArray *)label_buf->data;
+
+    av_log(ctx, AV_LOG_DEBUG,"CLASSIFY META - Face_id:%d Name:%s Conf:%1.2f\n",
+           label_id, array->label[label_id], conf);
+}
+
+static int face_identify(AVFilterContext *ctx, AVFrame *frame)
+{
+    int i;
+    InferenceIdentifyContext *s = ctx->priv;
+    AVFrameSideData *side_data;
+    ClassifyArray *c_array;
+    InferClassificationMeta *meta;
+
+    side_data = av_frame_get_side_data(frame,
+            AV_FRAME_DATA_INFERENCE_CLASSIFICATION);
+
+    if (!side_data)
+        return 0;
+
+    meta = (InferClassificationMeta *)side_data->data;
+    if (!meta)
+        return 0;
+
+    c_array = meta->c_array;
+    for (i = 0; i < c_array->num; i++) {
+        int n, label = 0;
+        float *vector;
+        InferClassification *c;
+        double dot_product, norm_feature, confidence, angle;
+        double min_angle = 180.0f;
+
+        c = c_array->classifications[i];
+        vector = (float *)c->tensor_buf->data;
+        norm_feature = av_norm(vector, FACE_FEATURE_VECTOR_LEN);
+        for (n = 0; n < s->features_num; n++) {
+            dot_product = av_dot(vector, s->features[n]->feature, FACE_FEATURE_VECTOR_LEN);
+
+            angle = acos((dot_product - 0.0001f) / (s->norm_std[n] * norm_feature))
+                    /
+                    PI * 180.0;
+            if (angle < 70 && angle < min_angle) {
+                label = s->features[n]->label_id;
+                min_angle = angle;
+            }
+        }
+
+        confidence = (90.0f - min_angle) / 90.0f;
+
+        c->label_id   = label;
+        c->name       = (char *)"face_id";
+        c->confidence = (float)confidence;
+        c->label_buf  = av_buffer_ref(s->labels);
+
+        dump_face_id(ctx, label, confidence, s->labels);
+    }
+
+    return 0;
+}
+
+static int filter_frame(AVFilterLink *inlink, AVFrame *in)
+{
+    AVFilterContext *ctx  = inlink->dst;
+    AVFilterLink *outlink = inlink->dst->outputs[0];
+
+    face_identify(ctx, in);
+
+    return ff_filter_frame(outlink, in);
+}
+
+static const AVOption inference_identify_options[] = {
+    { "gallery", "JSON file with list of image examples for each known object/face/person",
+        OFFSET(gallery), AV_OPT_TYPE_STRING, { .str = NULL}, 0, 0, FLAGS },
+    { NULL }
+};
+
+AVFILTER_DEFINE_CLASS(inference_identify);
+
+static const AVFilterPad identify_inputs[] = {
+    {
+        .name          = "default",
+        .type          = AVMEDIA_TYPE_VIDEO,
+        .filter_frame  = filter_frame,
+    },
+    { NULL }
+};
+
+static const AVFilterPad identify_outputs[] = {
+    {
+        .name          = "default",
+        .type          = AVMEDIA_TYPE_VIDEO,
+    },
+    { NULL }
+};
+
+AVFilter ff_vf_inference_identify= {
+    .name          = "identify",
+    .description   = NULL_IF_CONFIG_SMALL("DNN Inference identification."),
+    .priv_size     = sizeof(InferenceIdentifyContext),
+    .query_formats = query_formats,
+    .init          = identify_init,
+    .uninit        = identify_uninit,
+    .inputs        = identify_inputs,
+    .outputs       = identify_outputs,
+    .priv_class    = &inference_identify_class,
+    .flags_internal = FF_FILTER_FLAG_HWFRAME_AWARE,
+};
diff --git a/libavfilter/vf_inference_metaconvert.c b/libavfilter/vf_inference_metaconvert.c
new file mode 100644
index 0000000..89178a8
--- /dev/null
+++ b/libavfilter/vf_inference_metaconvert.c
@@ -0,0 +1,190 @@
+/*
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * dnn inference metadata convert filter
+ */
+
+#include "libavutil/opt.h"
+#include "libavutil/mem.h"
+#include "libavutil/eval.h"
+#include "libavutil/avassert.h"
+#include "libavutil/pixdesc.h"
+#include "libavutil/mathematics.h"
+
+#include "formats.h"
+#include "internal.h"
+#include "avfilter.h"
+#include "libavcodec/avcodec.h"
+#include "libavformat/avformat.h"
+
+#include "inference.h"
+#include "dnn_interface.h"
+
+#define OFFSET(x) offsetof(MetaConvertContext, x)
+#define FLAGS (AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_FILTERING_PARAM)
+
+typedef struct MetaConvertContext {
+    const AVClass *class;
+
+    char *model;
+    char *converter;
+    char *method;
+    char *location;
+    char *layer;
+
+    void (*convert_func)(AVFilterContext *ctx, AVFrame *frame);
+
+} MetaConvertContext;
+
+static int query_formats(AVFilterContext *ctx)
+{
+    AVFilterFormats *formats_list;
+    const enum AVPixelFormat pixel_formats[] = {
+        AV_PIX_FMT_YUV420P,  AV_PIX_FMT_YUV422P,  AV_PIX_FMT_YUV444P,
+        AV_PIX_FMT_YUVJ420P, AV_PIX_FMT_YUVJ422P, AV_PIX_FMT_YUVJ444P,
+        AV_PIX_FMT_YUV410P,  AV_PIX_FMT_YUV411P,  AV_PIX_FMT_GRAY8,
+        AV_PIX_FMT_BGR24,    AV_PIX_FMT_BGRA,     AV_PIX_FMT_VAAPI,
+        AV_PIX_FMT_NONE};
+
+    formats_list = ff_make_format_list(pixel_formats);
+    if (!formats_list) {
+        av_log(ctx, AV_LOG_ERROR, "Could not create formats list\n");
+        return AVERROR(ENOMEM);
+    }
+
+    return ff_set_common_formats(ctx, formats_list);
+}
+
+static av_cold void tensors_to_file(AVFilterContext *ctx, AVFrame *frame)
+{
+    AVFrameSideData *sd;
+    MetaConvertContext *s = ctx->priv;
+    InferClassificationMeta *c_meta;
+
+    static uint32_t frame_num = 0;
+
+    if (!(sd = av_frame_get_side_data(frame, AV_FRAME_DATA_INFERENCE_CLASSIFICATION)))
+        return;
+
+    c_meta = (InferClassificationMeta *)sd->data;
+
+    if (c_meta) {
+        int i;
+        uint32_t index = 0;
+        char filename[1024] = {0};
+        const int meta_num = c_meta->c_array->num;
+        for (i = 0; i < meta_num; i++) {
+            FILE *f = NULL;
+            InferClassification *c = c_meta->c_array->classifications[i];
+            //TODO:check model and layer
+            if (!c->tensor_buf || !c->tensor_buf->data)
+                continue;
+
+            snprintf(filename, sizeof(filename), "%s/%s_frame_%u_idx_%u.tensor", s->location,
+                    s->method, frame_num, index);
+            f = fopen(filename, "wb");
+            if (!f) {
+                av_log(ctx, AV_LOG_WARNING, "Failed to open/create file: %s\n", filename);
+            } else {
+                fwrite(c->tensor_buf->data, sizeof(float), c->tensor_buf->size / sizeof(float), f);
+                fclose(f);
+            }
+            index++;
+        }
+    }
+
+    frame_num++;
+}
+
+static av_cold int metaconvert_init(AVFilterContext *ctx)
+{
+    MetaConvertContext *s = ctx->priv;
+
+    if (!s->model || !s->converter || !s->method) {
+        av_log(ctx, AV_LOG_ERROR, "Missing key parameters!!\n");
+        return AVERROR(EINVAL);
+    }
+
+    av_log(ctx, AV_LOG_INFO, "\nmodel:%s\nconverter:%s\nmethod:%s\nlocation:%s\n",
+           s->model, s->converter, s->method, s->location);
+
+    if (!strcmp(s->converter, "tensors-to-file")) {
+        if (!s->location) {
+            av_log(ctx, AV_LOG_ERROR, "Missing parameters location!");
+            return AVERROR(EINVAL);
+        }
+        s->convert_func = &tensors_to_file;
+    }
+
+    return 0;
+}
+
+static int filter_frame(AVFilterLink *inlink, AVFrame *in)
+{
+    AVFilterContext *ctx  = inlink->dst;
+    MetaConvertContext *s = ctx->priv;
+    AVFilterLink *outlink = inlink->dst->outputs[0];
+
+    if (s->convert_func)
+        s->convert_func(ctx, in);
+
+    return ff_filter_frame(outlink, in);
+}
+
+static const AVOption inference_metaconvert_options[] = {
+    { "model",     "select tensor by model name", OFFSET(model),     AV_OPT_TYPE_STRING, { .str = NULL}, 0, 0, FLAGS },
+    { "layer",     "select tensor by layer name", OFFSET(layer),     AV_OPT_TYPE_STRING, { .str = NULL}, 0, 0, FLAGS },
+    { "converter", "metadata conversion group",   OFFSET(converter), AV_OPT_TYPE_STRING, { .str = NULL}, 0, 0, FLAGS },
+    { "method",    "metadata conversion method",  OFFSET(method),    AV_OPT_TYPE_STRING, { .str = NULL}, 0, 0, FLAGS },
+    { "location",  "location for output files",   OFFSET(location),  AV_OPT_TYPE_STRING, { .str = NULL}, 0, 0, FLAGS },
+
+    { NULL }
+};
+
+AVFILTER_DEFINE_CLASS(inference_metaconvert);
+
+static const AVFilterPad metaconvert_inputs[] = {
+    {
+        .name          = "default",
+        .type          = AVMEDIA_TYPE_VIDEO,
+        .filter_frame  = filter_frame,
+    },
+    { NULL }
+};
+
+static const AVFilterPad metaconvert_outputs[] = {
+    {
+        .name          = "default",
+        .type          = AVMEDIA_TYPE_VIDEO,
+    },
+    { NULL }
+};
+
+AVFilter ff_vf_inference_metaconvert = {
+    .name          = "metaconvert",
+    .description   = NULL_IF_CONFIG_SMALL("DNN Inference metaconvert."),
+    .priv_size     = sizeof(MetaConvertContext),
+    .query_formats = query_formats,
+    .init          = metaconvert_init,
+    .inputs        = metaconvert_inputs,
+    .outputs       = metaconvert_outputs,
+    .priv_class    = &inference_metaconvert_class,
+    .flags_internal = FF_FILTER_FLAG_HWFRAME_AWARE,
+};
diff --git a/samples/shell/face_detection_and_reidentification.sh b/samples/shell/face_detection_and_reidentification.sh
new file mode 100755
index 0000000..c09432e
--- /dev/null
+++ b/samples/shell/face_detection_and_reidentification.sh
@@ -0,0 +1,131 @@
+#!/bin/bash
+
+set -e
+
+if [ -z ${MODELS_PATH} ]; then
+    echo "please set MODELS_PATH. e.g: export MODELS_PATH=/home/media/workspace/tests"
+    exit 1
+fi
+
+BASEDIR=$(dirname "$0")/../..
+usage="$(basename "$0") [-i <stream>] [-options] -- program to do face reidentification
+
+where:
+-h            show this help text
+-a            use hardware decode to accelerate
+-i  <stream>  set the stream path
+-s            to show on the screen
+-v            to show debug log
+-d  <devices> set devices for each model(C-CPU G-GPU V-VPU H-HDDL) e.g.CGV"
+
+if [ -z "$1" ]; then
+    echo "$usage"
+    exit
+fi
+
+while getopts ':ahi:svd:' option; do
+    case "$option" in
+        h) echo "$usage"
+            exit
+            ;;
+        a) hw_accel="-flags unaligned -hwaccel vaapi -hwaccel_output_format vaapi -hwaccel_device /dev/dri/renderD128"
+            ;;
+        i) stream=$OPTARG
+            ;;
+        s) show="true"
+            ;;
+        v) debug_log="-loglevel debug"
+            ;;
+        d) devices_pattern=$OPTARG
+            ;;
+        \?) printf "illegal option: -%s\n" "$OPTARG" >&2
+            echo "$usage" >&2
+            exit 1
+            ;;
+        *)
+    esac
+done
+shift $((OPTIND - 1))
+
+MODEL1=face-detection-retail-0004
+MODEL2=face-reidentification-retail-0095
+
+CPU=2
+GPU=3
+VPU=5
+HDDL=6
+GET_DEVICE_ID() {
+    case $1 in
+        C)   echo $CPU;;
+        G)   echo $GPU;;
+        V)   echo $VPU;;
+        H)   echo $HDDL;;
+        *)   echo "" #Unknown device
+        ;;
+    esac
+}
+
+PRECISION_FP16="\"FP16\""
+PRECISION_FP32="\"FP32\""
+GET_PRECISION() {
+    if [ -z $1 ];then
+        exit 0
+    fi
+    case $1 in
+        C)   echo $PRECISION_FP32;;
+        G)   echo $PRECISION_FP16;;
+        V)   echo $PRECISION_FP16;;
+        H)   echo $PRECISION_FP16;;
+        *)   echo Unknown device: $1
+        ;;
+    esac
+}
+
+if [ ! -z "$devices_pattern" ]; then
+    DEVICE1=$(echo "${devices_pattern:0:1}")
+    DEVICE2=$(echo "${devices_pattern:1:1}")
+    D_ID1=$(GET_DEVICE_ID $DEVICE1)
+    D_ID2=$(GET_DEVICE_ID $DEVICE2)
+fi
+D_ID1=${D_ID1:-$CPU}
+D_ID2=${D_ID2:-$CPU}
+
+GET_MODEL_PATH() {
+    for path in ${MODELS_PATH//:/ }; do
+        paths=$(find $path -name "$1*.xml" -print)
+        if [ ! -z "$paths" ];
+        then
+            PRECISION=${2:-\"FP32\"}
+            echo $(grep -l "precision=$PRECISION" $paths)
+            exit 0
+        fi
+    done
+    echo -e "\e[31mModel $1.xml file was not found. Please set MODELS_PATH\e[0m" 1>&2
+    exit 1
+}
+
+DETECT_MODEL_PATH=$(GET_MODEL_PATH $MODEL1 $(GET_PRECISION $DEVICE1))
+CLASS_MODEL_PATH=$(GET_MODEL_PATH  $MODEL2 $(GET_PRECISION $DEVICE2))
+
+echo "$DETECT_MODEL_PATH"
+echo "$CLASS_MODEL_PATH"
+
+PROC_PATH() {
+    echo ${BASEDIR}/samples/model_proc/$1.json
+}
+
+GALLERY=${BASEDIR}/samples/shell/reidentification/gallery/gallery.json
+
+if [ ! -z "$show" ]; then
+    $BASEDIR/ffplay $debug_log -i $stream -sync video -vf "detect=model=$DETECT_MODEL_PATH:device=$D_ID1, \
+        classify=model=$CLASS_MODEL_PATH:model_proc=$(PROC_PATH $MODEL2):device=$D_ID2, \
+        identify=gallery=$GALLERY, \
+        ocv_overlay"
+else
+    #gdb --args \
+    $BASEDIR/ffmpeg_g $debug_log $hw_accel \
+        -i $stream -vf "detect=model=$DETECT_MODEL_PATH:device=$D_ID1, \
+        classify=model=$CLASS_MODEL_PATH:model_proc=$(PROC_PATH $MODEL2):device=$D_ID2, \
+        identify=gallery=$GALLERY" \
+        -f iemetadata -y /tmp/face-identify.json
+fi
diff --git a/samples/shell/reidentification/README.md b/samples/shell/reidentification/README.md
new file mode 100644
index 0000000..f52ae2a
--- /dev/null
+++ b/samples/shell/reidentification/README.md
@@ -0,0 +1,38 @@
+# Reidentification sample
+Sample demonstrates pipeline which uses several neural networks to detect and recognize people by faces in the video.
+We have to create a file with features that are obtained from the classification plug-in with identification model. These features will be read into the gallery and then used to identify detected objects or track them.
+By default, the generator uses model **face-detection-adas-0001** to search for faces and model **face-reidentification-retail-0095** to create features.
+
+## Creating a Gallery for Face Recognition
+
+To recognize faces on a frame, the gallery with features for each person should be created. Gallery can be created using supplied gallery_generator.py script. To do this, you need to create the correct folder with images.
+The script uses the **metaconvert** to save tensors from **classify** to a file. Parameter **method** in metaconvert is used to create the file name. This file name is defined as:
+ 
+1. image file name - if image is in the root folder
+2. subfolder name - if image is in the subfolder
+ 
+Use following command line to run the script:
+* **python3 gallery_generator.py -s <images_folder_path>**
+ 
+After running the script a folder (*./features* by default) with tensors in the file and also a gallery.json file with description will be created.
+ 
+For example:
+Input images folder structure:
+* images
+  * person_1
+    * img1.png
+    * img2.png
+  * person_2
+    * img1.png
+    * img2.png
+  * person_3
+    * img1.png
+ 
+Outputs will be:
+* features
+  * person1_0_frame_0_idx_0.tensor
+  * person1_1_frame_0_idx_0.tensor
+  * person2_0_frame_0_idx_0.tensor
+  * person2_1_frame_0_idx_0.tensor
+  * person3_0_frame_0_idx_0.tensor
+* gallery.json
diff --git a/samples/shell/reidentification/gallery/features/Basketball_Player_0_frame_0_idx_0.tensor b/samples/shell/reidentification/gallery/features/Basketball_Player_0_frame_0_idx_0.tensor
new file mode 100644
index 0000000000000000000000000000000000000000..9a7eb337cba013159e27ad3d3b52d886d035ce8d
GIT binary patch
literal 1024
zcmV~$3sBPq7y$5zo&>}xpgbHUuMV8TVael=eIHjBA%+?Ug`Js(5g8!r0YZrbnF13-
zwvdOsjE7U=A<`l4|9#6@lz?92B}g15=#Y*;#1Ss*_p>ps#C;wy!br0h-*weee!HCi
zbSUIkZ3yQ!Uf?~-35eG0=d`F=SX5_Yp>dct_S_ZXJ)4B<ss{AYhhw@di!4$b@y1FO
zhn+nQkw={9SA8D$d$gk|unpqHDzs1r^6u_RT<7)=Bzo3x)<PuIo%|aAJ=M&ejS3u+
zf5$~7Rq({0K_8b-8f7KDX7uKRsd7wdcI5q+mh+}&GY*-W(|o+ez}hZtaLc=$AgekF
z2^`9a2{P>AvrMWE^q#DwPx|{ncGR0EA9wOlk(|2iW>EISdacG`8Y-;^K@uoumHIqd
z+KNeCwv02*EP=~x7I#LKV!WXY1E3tP#kWZ-o)!`PEJAJK_t+zT1K&T;#Ez*F9^6vP
zb_H6ntF6Mkk4&J|CI&|&WU#_7n?oKJlID*gA^bgiY%QCo;v#Q$$@W9qY{4;yW(7AZ
zDW#Vtqx*?9xE)i-Ow+>MMm4^o-XOgd*Gn8{#(g=yEVk~aze9C=&)|&JsRF*(JtkOP
zD&Vi&%RwE~L|$)JbLVX(&$!z2qW&tKQ2$6?(@tnR8jE?xO*nDW2Um_*3J)Kw3i(ob
z0ZSc4?B}wYm(P`hNbk(W`o~nW!#w2v;RrNaTR@RXD|qR@CTJTNp#?=Jg06y(><=Zw
zNRJ?h2l!=-jGi8n@V&@KY_!kA7y<C?*r#mm6U|PsPMBj#rstwu(6^q)T3<WPkNg7X
zF0N*kBn*a4gF?*VGdKFj8zC-J$;l<3LEPaUsh6&pdcKL}boUJ$8Jq_0Di_#kqu_Jm
zG^`Nc2ZP3yQ+>Ro{hQN4G-e8#Ygr(T<uLYL@1Q&8{+JnChidyQddF7|#r{QL-g^~7
zD!sVy;ZN|)+*asNgrOp36L)ME;fm~B(CrP!n&f&uSewFr1&c7QD3*q`euxeoYRq$e
zL7#7}fzSXc%pM<w!1-3#x6TP4^#{<gjCx9_|Ca8Y7^lmMAUv3N1*60r)EjY&=BHYu
zmzsVPsx5D0X~{2O_tqpW3p~w%f+w|<q)E?JIB2hZ;D)MI06PXlakRaQJ^nJ%_!TWS
zJkRC5Z<o>H%K>i89HZ+K*6h6A3N9!2QsFZRoHD8Dm`?ylOoiivCo;(G>Vh+?4Yce2
zMRJiRN!PX&@y;?8YNxmHaa|5-bTfEhP7F)a(PSej!#$&Gut7P%bt4PBHOYs&4gPd<
P+wU}+vkP878qfa&+SCkk

literal 0
HcmV?d00001

diff --git a/samples/shell/reidentification/gallery/features/Basketball_Player_0_frame_0_idx_1.tensor b/samples/shell/reidentification/gallery/features/Basketball_Player_0_frame_0_idx_1.tensor
new file mode 100644
index 0000000000000000000000000000000000000000..3fcfb236bd1a764deb8f7369d22dda4f4282cb75
GIT binary patch
literal 1024
zcmV~$2~d;;6aZkZNf6`^l52pC5h2tGl3+3Z_fQEz8g*H&b_|eYG)dUf7zC!mVnAFF
z5pcLhk)xJ-Y6Zq|-=hNL5J5$*I9%j5%2Hu0puVp;<rixBP6;)E&#63G8w<M_dU7PJ
zH63-Qu*DQpYhpQ3*PW$btbzD^Bbq%cq4bk4IXK~4==5vg3(|9}Nv?%8{}uH%u>{}B
zEuueqdPqw*#Yf{AhC5zoVTdXNr=RqyFF(D8`+u`Wd6)^fbQDqiy9ub3t;-(m1?t%F
zd9d$40^Z6Z5PLaNZ?->+tbR~WTCc-pziTw|OhI8qajbeR!W$3#>D7iO9(1$hfWzUi
zzWN~=XugDy9V=89@(800nsKXHIes&m;^TSX1~ptUL3&fb-xqhGVZJ`r4J^Y^r&elD
ziH0-cO%xq)3YMLWQU9Kli(6iStg;)DJaXV>WC-PrA4AVXYo?STiWttrQ#+*Wb!#8h
zZYK^%7SVBGf}kpm#i<Xd^kN*ZT~+fxi^kBQ<A}}F3|E~k(LGNai>FHI>53&j?#QB%
z1s!_0=8K0n1RT$=7fK}U&>9>=v_3`iElOMw#AKzTq$mA(n6EVDlik_q+oQy(otdPk
zuZ2f%){(=bAv#wvr1qDmf$eoIJXB%GHZ?9-Z5RpnrAxw0MmCj-EAiGoXH*#+hX~vP
zviD8-ai&Sz+}o@4tEDRsjGE(}1Kt<~yM293Rg~r7!sX?cATekU>cv(;anKyOCFEda
z#|hR-rQ4_HG-wpB7B;2J_@mG??)kX@-HbN`dqp%Gdsku8e5hI!xd3L3Wtba&p5{in
zz*Nsx2;9G)9X~&YpfF_PZfA_TtKg-XB&OZdr0+4#Q=1n_Qkluy_BgP}#~mft?^5*F
zS;9-|B%6r_9?%59J)1WK$5dTxmgS;jn*|436c8n6sXuMYVbzNfXg8Wgd&LAzP887N
zxqjGu6KQZv;O>vDU~j!#z1pjyUcXR`JrlqWHFg{ppv{XHW?*KIJ6_*AL2~m*b#%TL
z8WsCt48%i3ax%k`E+#c)pleeWxk-1z_bLNS4EvMQYt7+AqZ5?;`vLBi^unl9Ck)z|
z;~~3h&I>FQ#;iNo=Ik2T9;{$hSr8A_{Y7T}eIPm_#kejxX3ACUaN2+;^Tr?|yOl@6
zGs$B16V5l0plZdcZQ1oAuj5wq-8BgE`aX(!r;4A4&B7JI0{@<qK+JLrSqXtWT(JcU
zI~{4kSwWi5GWb6cvTVB(n4B#{W0?=?{L~J&CmrC$iZjF;RpYaHYaURlsqe?HaHQlt
JzCT<>{{yBm@$CQr

literal 0
HcmV?d00001

diff --git a/samples/shell/reidentification/gallery/features/Person-One_curly_hair_boy_1_frame_0_idx_0.tensor b/samples/shell/reidentification/gallery/features/Person-One_curly_hair_boy_1_frame_0_idx_0.tensor
new file mode 100644
index 0000000000000000000000000000000000000000..b54ef1a6338a600680cbfcb03490eab30a395405
GIT binary patch
literal 1024
zcmWN_eOSx~902exk(Q--uJuOlIjfB^+*+l(-}h@OdD51iPFgL`77?P`lTn^j==HkN
z%X(pT>xvH5oUJ16_x*avgoxH$ULNHw$sW<3y_C%M`STN{Oh8-PEbM%fgp1RZTw6X9
z;~Ycyij5IHTbe<etmV$EK+GJSPfHaignwFnX<U@BPnrTfUgu(tp$QV-w#RBq5gS(I
zQ~K&Vutsm8?Pro;TkjCH99++6tifed1zy>IpQ<c1C>On1$^y>&*I#^LeO?F%n0_|m
z^={O;1YyuvJ9X8Skc;aewc!#@G3Z1~)FrUfMRJt;c46=rCC6=1Vdw3q;>DA9IPFVE
z@r&Pn<PE)USgBcn&Ur@EyGMgQVic+ZoLFu@8z(#t0$m*ib;bl~lAEz>|B%pd_98^*
zxYN1|!<f10|7$EUJ6WkIu6!?tZyFc<YP#{kwy9`U@fo+BZV<9MPLM<BE0X{2f~K-$
zx|w?$LduHST+vM_A7~(SauF^wT}9dQ!*Eya${RX51hltDy>GV4N%=lcKeZa-+YIP!
z3FZe^ym;v4DbXwPART(1Ev66OqP?lWYZFExPnjsZ^k{*EErn+P9a5~9>Ch}G6BFjP
zb4^${K8L-y`$hq7Qy}N+WVl7rMNekWMaSYC%6S|CVa{*y$bm$PmfK;g`V*|KcSZfp
zRQUc)GDP|ops%!slM1~l{z*1ZwyPGh(;ex?q(+=_I}2fB6KMwg@Jyi>E?QXvHkBu7
zUWY$JNeo=~I?glae+-xJJ;XnIQnBS+IJ=dTIb+6QF=@&UURb;ib&C#wUgt-BTjzjh
zhBNlde0cMu9#~s^iB*BigyDArvFX?Vu**u`_@or$him!8b$?jZVx-eC`IOKdMg!(n
z!D;savI+hf``n)qMI_R>trFh3WFL)uWJHs41KI3P0okv|VQiYi(&wc<FxB=`L3KWu
zwd#GWUTlry@eODiX{1=I6)5fh0b8w_@LOjcWN55WAv}aGPdBu3s1o)#n_<Rl6S^hV
zvQ_L0EE_z;4R%GM>D+P_j+WznnLE6fJ0v#0n@`5rJ#Zv{A$D%A5K^xNu<Vs1PY+sz
z9zG8U2mb(-uU0H_>W3qP4#KMOI9{Lnl6x2W(dF3d_{i%P3^k95&nh1gEI7vBiS=04
zXTbc*Ogw*Vfa+pqVP5|j$PX8Cdh7+Tv>l~2l5PsA{##Udf5V}6Pib1d8tO7X2bh&c
z_S#$E-E##L_oWyb^%Fh5I|0KP{;Vq1!mh8+Qp3(Zyf!BncNs?Uul70!(MVC}BN46T
UDXjhqDe}WH-d>>~&w_jOA1-k0G5`Po

literal 0
HcmV?d00001

diff --git a/samples/shell/reidentification/gallery/features/Person-One_pink_hair_girl_7_frame_0_idx_0.tensor b/samples/shell/reidentification/gallery/features/Person-One_pink_hair_girl_7_frame_0_idx_0.tensor
new file mode 100644
index 0000000000000000000000000000000000000000..39284449564fc88eaf5f0c6984db835a7c8b2a6a
GIT binary patch
literal 1024
zcmV~$3ozSv902ghygE*?qn_0p)0&p(f*t<9FJ05AGh7MnXq1~3m#&<8bRM^QWGN>i
zp0#+aU5l>OEvhj|eqZh8u`*TTR>R7!P0f{gjn$&|`D{vlq|Y|z$@t8tq(pTXbY*c&
zzAqP@lU9f>c1-hfss$Lk*F-m&g>3Up<}02V@DBu`{Q5S15_19uno6l5XFo<KTGBK}
z1NeNg0mKOpn5O1A!g4ghjBys6e#nEfeM%5a)X<TUSu)yZj{-jz2;A2Bhu=C!Bo#3r
z4$H^YbR&#=ln%Y+wJNh}C1K4u5x(^X+%p>_ai5t&Y5YYZom5c$JRA5{eF#3${7pmT
z9Gs5KM`8PIwDK_k-7h*=3CqRU{KuqAQU{$m|B{49PSD4mq@^)zkh;W^M9l^@k3Ry+
z^98tdVv!L=2}w+67p*Y;fh_ab7#zicfE%}nPj)S6+b2kIoI7&cEP-h?#?bxYl&vnJ
z){=F6OSnLz5(=4p`o|D@TOr5r2JE)r<BN|{sGfZV5q7LFuK%$iVwVTn+Vs#Ps%-Q-
z7f#+g{imir@@w=?F(OXeBQ$hMfc=eb_{Emof7;s;v;)P=sgg9>Z!*fvylO>qvmWL0
zO1$Y}OZ}H@F?iPm9bPYm%!Vv@(`bd{9Jhwoj@Qxks5!EPp?(Fv158Vp1n$2O(Yl=>
z(!=GzQTioxd#D(Wc2KqKmV{+Poj9hjz`Xm87~pvYe?NH@)Nju*Y01yXwZ<xNo)TkE
zW)b9_QQ=R5a!CI5ICF5w4vLSJ!$_)($TVhzCocq!G!{icAAlrcH)`DPlKG4xob}ua
z;7_+{nj4F7DMJd1uD@_A=s8{z8Gy)Cj6OT|xR&SD+mw++=at5owsjbvMtFhgOF!s#
zXd?M$i8R_>!La>?smbCRir7uik={(Ww)+UnMisDkPcM1ym5LXpu8@=!ADYa4gV`*3
z7ya(c&_abb2n1ppav=pQmj#e*lt#qU4)BhRGqg32LC0DQnpc;htD+V?9}iNK>H*>_
zWTD_dA<5NOG4sqJBG5aJ-xO#`s4xpdvi9Pj;aLzWboAs@Cb8aA4BGM{WK~x61oEz`
zTeQ}&{)duwEtDgFTuK8AzXNt}1GAA5PU@;m1B=#q=n~XOcdXtgc^-Nsqd%EOuj;{>
zVLs}NyVa`~PvL^v79@Q&aIoed8nGlN4h5!IyfzN3%k65ps1imWh|o^<Gp5fEk=p`K
zs;9Mru<x_+<GXygr&d$hZ)$iVmH{W#LSqy16s9lbGRgIBXx{cA1W%f2UhTUE1rP5~
VX?!FoeR*IrMsZfgMW+xc{10w|?&tsj

literal 0
HcmV?d00001

diff --git a/samples/shell/reidentification/gallery/features/Person-One_yellow_hair_girl_5_frame_0_idx_0.tensor b/samples/shell/reidentification/gallery/features/Person-One_yellow_hair_girl_5_frame_0_idx_0.tensor
new file mode 100644
index 0000000000000000000000000000000000000000..8829aeec63879131b86015a18710bc77d7a36262
GIT binary patch
literal 1024
zcmV~$4KUVs902eqx8$^t*Sr>M7)J5vDLwz+cXP))F>?*KIZ=paC-rFEv!@IXV`DvD
zCwV(D#lsWlIwzjz`<3@zl%C8T!jre`tYV2;+wAkXpj~E991?(c{|2xhK1WT07oPvs
z8tgVpP_!I~!CQ@3YlW3CL}H7d&M=DFUWqeS^)TPI00UvAI9(J#t%G;bEMN$Fu9m6o
z2khabC<%i9_>9>kJXgQrH3<XK<~T?5UUfmjkL=uSAF40^u={zE5$MnCVy9noq_8xD
zp7=CMkLna2w_7KNyK6#I-41n?)3jRad5;_Mw!luu92Ob3jV4?NaQ>Bub99bDG%Q1Z
zO)SVfa_K;A9*f^!3V-cLhwIP6p}NuypU5mB>+eX~x5pk-&2{+nbsZZE^9OlV9Xt?y
zpgxrPB`LdWLF<-F4TF>1!cs9_yV}WK7A3=Xf_mKeO2$;h!`$#46-|hfnC&hl{^?_h
zqa)#zVb;ssi#X^NJ;KDm$5b?&4`Gqj^hshhYd_bC$#n`G_1y_IsS#)_)}Z83HdlN)
z8{WCK(|X`G3dl(juJJ{%9Iemk`1r6w=>%lfkF(I_PHd<uVDgyD6xC6M$C_-&$kmV<
zP2HeRaFJ#2-^(_P4M<)dPjmkFn2XLC;;oq<E;z6iI<h1<<+}<tx(b^2YZ0DEHi3~?
zIXNqOFs?tDl^B(?_kJ(rGLLoO{f~ZvJb5(@nOK0kMuLk|4d@x@$HpQq3k_t6;MAVZ
z9Jv=*)0GSTK9j`L6mwY_0`LmRgY{<>*x_=AwMr&bt8d0gnln!BEmN5Kbc}pfMyOn@
zWlLvESyQzq*@t+6d(Afb_v>rOYlvneHQ&?2<~h7@YY-FE70`XA62&DN(2?z?m@*MK
z_mo3}qZXR{_{9Ih3rq)N@Ii<erW<9HR<;A2K#9EYRWyz$fo2nH(!Os<x_k1-V^RPn
zZDHUILYNu|ri&F@;6<pEIY`^lMV`s*QgcA{tpa1oTWxIq8>qT-$oHm<R`PRH!hRFd
zt5Y(aky5f9+6lYgIe?DT7e*)LV05JmuB<OGbuN#(4kAp<meEQeA13-*QJO4eGckv#
z#Xbmv)CM%GmkE6v?Xam6ii@*))cx`;9$8efo8PRGfBYh~imXAHQNo>?JfgnsI*xlE
z^VPNg{D%((J;=9E(!N_(aP((==+opwz?Sp4CQf55jhkFW><xTd=>R<q+sUP>5$CSA
zalS)KoVm3Fl%#N!a!LTN+>U}IX^y#<nnB3%W)>%2Vl~5ug^T`$<Z<Z)Tq+%9MQ2Zf
TV)A2%TTxQ_FVR$UFo^yK+Ee<q

literal 0
HcmV?d00001

diff --git a/samples/shell/reidentification/gallery/features/Person-Two_man_2_frame_0_idx_0.tensor b/samples/shell/reidentification/gallery/features/Person-Two_man_2_frame_0_idx_0.tensor
new file mode 100644
index 0000000000000000000000000000000000000000..77a14e3d6f7d223d5c8d5a698ba8d2ffee6a4a63
GIT binary patch
literal 1024
zcmV~$4LH<y902eeC2!+-s$DWs)7@BkMq!-$|GtM_#26Bb!{x1Q=4BzAm+o#WH@jiF
z=PFHZ=1P=VGoJAOeNVP$9!XTTE2qbVHhDx(j<%<^&!@K33HJZTvd3e&xTo<5>`PpM
z;?!DX?`}eI#Q}P5o0WbjNC3+>?NGm&M;F%zVXnOtQ`JQ<xsihl8xts5n-84UokC&2
z*C^W(z<%f?Wg3sVv6|!sI={CD7aukfr>-s7H<b($t8!YaQ{a5RFZgieWK19ezJocP
zTGbFI=R|s7Ivibd-+)5FZknU@qb5}YrM(_7V8zwX3NO=%$2@HOYZz6%TI`U{kYPsy
zJ)8Ry6=uOu>EZ+0!WV3-+h>@cAZKEBi>WM2NIUPvk?A%07+3HEdcWcmWsD!JMm5vr
znnhL+R*f=WHR~)bWmA02^k#A%v@Ps_Ka^usvt%Uk?nvm1TR1q`$2#2S6SM3z%(!%$
z@OE^gAjJ}PZJi}HUSe2&q<~HRav~`xgG8=Bo~YN*?dB3@Ux9&+gvR1vIghkBYeBEQ
zi_JkwaIg0Qo4I7n$g6^3XD^TfM>I2j0hivZgqrSBkPn5R?c-G<+w3FyG3Q@e#*4+u
zsz};xzJXDOa?o{u3|Q>pf??7~#qMpiB;5lJmZg9b6Ao_Q#lhT1g&+<*#K`{%Aeib+
zGqak|(Ju`+l2~@bjaDqTn_@gqH_`X&0^r!GHu`)2O^lB+kcYan_&mCUO&U)E#}I!|
zFAl?%Kyy%L29d2hJy~%yhpiXLsDmh!d{b5h(@WmW1N8*<Bt3_b^w*hj-v+2|QQ<F7
z<8c1UPgI{i#HwFAN;}CY&9NMXcMh)5rom=Z2yL<Zdw&`hXpimYa&$By>$c_z$#HIn
z3pT+}b|L{1DvXrhpo6-r*C>B33+CVc5;9jm1+PIdcFL@=sOfVQ=C<P0N*A3?dl!mm
zAKZ#K2DX>9*kU6jhVXJY|I8Uf8jOS&F+qxFu9L`%lZ<b8BHXK13f+Vn=2Uz%Y5!A-
z9VRC-)Xx#64y;1Y#XNFH;)2DA_26Py2cqrA8TFH3G!Ai*EUtylDH%=4P}2z4a@6Uf
zz+><*ZF~ANYb-GrnYiaM!)`qk{kj*ddG~2V_P6j``6tvO@-$QwtpiP6544|&2ft@;
z5ewXfW1~G_Yw95bS()(ihBbb0s2OD;VPrnc7jIw9M$x0rfLCoMxO`@Wd_4UXYHM0)
z)sbx=QZ@AcEy+V|u?|N@1#FL{y{Mz>Fe9G6#uR$mf~xWhoEd);I~B21Wl=z7%_YJ+
W{AKc>;5yZ%o`cldlT?v$lKc;t|M6b{

literal 0
HcmV?d00001

diff --git a/samples/shell/reidentification/gallery/features/TomHanks_8_frame_0_idx_0.tensor b/samples/shell/reidentification/gallery/features/TomHanks_8_frame_0_idx_0.tensor
new file mode 100644
index 0000000000000000000000000000000000000000..b42345d583c0cc393594e25bc132cd5d92b1cca1
GIT binary patch
literal 1024
zcmV~$4LH<y8~|{Sav?d69&c-`+1Z(GUiN6&{eR!f<SFcNBIR*2FFi8kZN|LZlWR5a
zmo84Bb7)g~vcojd{k}I^ZdNL{Vn|V5pPRLrNUD84<sq9ulT!#gz0H9gWG_&SbYmIo
z8i+>o;f;$asB7h6-cK^fsCbMzX(4Ih_~2MyGG=bhCBE^0GrZ3OApNue=c50DW0zPs
z|K)BRkr`@AlP_Xg#}j&NHG#}cR*+1WDd@dB45LxGXlh;smElhC@x-#aDqRAWE+=5I
ztet3MJv5Oe9q4}TAelFwqN?lR&=7BgL8WqHkyydB&#LHGU9TCDz8|wkSLl`q5%FFK
zR)=oegks?-oKRhXPgx4$Ywe0V$3uxjj2Vzv2V~{$#f|bS@U*BDZgR#jDvgJBQ|EC}
z!DnRb+v@x*DI~L!K-yM|67D3G&#s`sktMpjtOH~c31~OSX}3=&l;oJhH^G5azhi*d
zKWHN_<pDUt-GY978s;N12fs*+K!G>NkWnHvc)g6ha!Z)g>Tsj(G2vv~CblwDn6g$-
z<)|3Cvut4fdLP<FI75EUNoc$yLT;BeCWg&Zrz9!-s&6EJ+Imv2M}~s155plP+X|0z
zYv4w@C$8JH;vw;SQ1kXM3T~Cs%7qd-vM*MhTHpYR;Z~CDHOJ^nnrXsxC?4jY1?6vk
zu)nwo*7K{Wqf&&?|FQTP@g^8%x)}xL_C4DlXi-$%!Z<gk5dC@{8EfM~n<E>39P`5T
zL^1rlGJyw=?t<gZQS{r=B3kqC9+C2fsLl5`>C3oG++!kV>;m`FwSTR^0wRI0<Y3rV
z3k->HN68sWG+X&y9b4>8%!F*RSbP>Y$PZy!k_TCuu7K79Con!kiyySuQ|=#1+|lC<
zvN9(e;H*>D(v&(+I70<%Y*O_i20aUmaC~$CIraCcR?$upN?e)uUs%C~t|pq)ei!6E
zCvep#j`5Hc;Z>JJEIKbm)#Y*ssQ!V8m#RrFrvb&gIMm?zC&&p5q1-hk4mu42IlLcd
zME9Y;J^({!8JO}t4dOa~sGpD#UbzZ{i(lZrJ>giE@fJp(i-Wp?CCFoUVDC1Dx>dN~
zcc(gWqS7A?NLtS$v>?{C&#_h!4VT;oNW+oskZO`m2O{O5(S*^onLEVq>KU}y+E0Z6
z2E>Y(!uXSBD*rlxRAn_%8(kBvd-xgD*WN<anxyBQ1F>M96ATHy?qqev9Zyd7LA|b%
zS@5?dLrH~bDRL8dJ6=H7YD0m*{ayY-zmmLqGZ~D}J>y?)2}a}8X&hYk!&fi_{ZId4
T+9W~<(#vr0***1fb`1OvlQsEf

literal 0
HcmV?d00001

diff --git a/samples/shell/reidentification/gallery/features/snapshot_COR_VAN_DER_KLAAUW_3_frame_0_idx_0.tensor b/samples/shell/reidentification/gallery/features/snapshot_COR_VAN_DER_KLAAUW_3_frame_0_idx_0.tensor
new file mode 100644
index 0000000000000000000000000000000000000000..6a26ba7daa47272bb44158ee4a3efbea1367f0a2
GIT binary patch
literal 1024
zcmV~$2{4>z7yw|l7!ui$xY;(XtZSO7BZ%w&UM6WNX{xC>${1Hz>WXN?Hra6{A>x>2
zHIXoJtRtaAhpv|YeI><EYLqIIv^(S=QfVU<OcL~YW)}>hAl3%9|I(nj1N-QV#a)P<
z@4>Ot3Va@34k6*$a3tKH9HXUBv`0v}!E@Z=QV!(x=)tw_@9}uRD0fDI*buUV?Y?rp
z+gt{<wn_?mX2O@LLaFnl3m9|=339E;(%%4lEDhn2X%6SGB_p@%LMoXOlgWk-2~{`2
zDB?Ym<`nU2X)=u2<<Uq|0dtBM(A=gj$WPyZ^-u*4g|$JhtuOqRGfXLB2{pdfW+#Wz
z;gzn%K=Yu0#C~af({wcj_kBUlNj6{__aUrCyOMUe5^GTt)YfruZ0anhcp3<XESiPR
zso6*N0uo^gb*k-nE8ik^B=#O2jFfPa2oK1*XAMVJLNLBKhDGTIvs7kF;)e%WXiqFz
zf1Xag0mScY)zQ}dXwn_B<t6eO>>ez|>3RVf4yHqsmkt$F{mz?}O~ItDgjrTs()i6r
z@EO@hOC3!t-z%R6_HwYDse{~|7Ah;&Bi+VQQjbfi`;QTp=~+Vq_yfMUoroFLHr&=_
z0Zjg<!R1yB_f?68nZDu|k}mtwoK7(dXq94&eHW&wLeQkr0-bi3z_?*Qr?Ob#vR6m>
z<qNlAvNDNN%PdLe*9a)@BD<J0T%V7Cie4{n-e%cb_f&rW%uF`@cw`s021k+0W-90W
z)dXa07}272F9`4W<CE-sSa~;(y;|}dxRPiJg~}ClUY+$$PZ7~2#bL^{>S5Y;T@W@|
zOY?DaeKR^Q*{H2C<#ZUqK=WGz$FUtVjt;Vo9S3h?&Yi}$gxJ|U#@60HOLhJ=(B+y$
zUpI*GC)&$e1UcYRJ&k|cOGpu1g2tL3(}?vcc%muCg&2;~5^r!Tojcy`b)(>_`i#td
z{n)n?4j}e?$!S@6LYVtH6Gac=gM}ap+oSK}=A4CQKQkEY3}WWI9<6;^K|X>%y{>-Z
z3mTD;eXF}}p}@)!mj7vCmnRF^#o$a<xH}TAD$8i*yaQ|-R6*$L3c`60XfNI57R?v%
z`MQj2KVZg8Rc(m-y(rN93MTyf0_}!$VDr~?hVluPXJtg+9ZmpoU=bd>+tL5dH<x&c
zlN*+w5P~dOOj9C9_NM#|mm4Rg55)JGhr@N!vQYAmqNAw(#5xnFDxsP80j)!6@K{tv
z=g%FY);1-&_P3z<b}w^@&xYg}5vXSyxM#Qav#f#^E+Q+NUE91s57#`fy}}(He0h$0
TdeMgkshp`K;W2w#R*(M!MmF)x

literal 0
HcmV?d00001

diff --git a/samples/shell/reidentification/gallery/features/snapshot_DAVID_HEMBROW_6_frame_0_idx_0.tensor b/samples/shell/reidentification/gallery/features/snapshot_DAVID_HEMBROW_6_frame_0_idx_0.tensor
new file mode 100644
index 0000000000000000000000000000000000000000..9a3307f0d84c0b6eab412db9ad1ec2668f2f2167
GIT binary patch
literal 1024
zcmV~$4=~jS902eao}5JwVa<8Zu2)&P2(gvl_xq;oUcyZbLrslN)HaXQl&JO2vvU6I
z)@00jnLj<Yq$YX4@3(keLo=^Bl{wiQXH=6_^g7)>pVk3ioJloP?q4OWznRQuW^DPA
zOFIQ9qXbD|S8!$Z1elx`(Q}tJ+Mf_Z{@U%hI_)R4I(8OZ<22lseG`LHa_Ls~29!Z3
z9J#WR?#uqAz1#kyKF4G>-bw=PnpU{}wmX%rOW~=WF*<E_Cp+!WMpJ2z_}f~6l^(x~
z$$`!AN*au=?tbX@U;@InhLV?Q4~Dm>Nq@;7GyjlsWqcQHSdgleigwCawwEgo*|U0G
zzIbKLpM22q4%K^yuzH^w)%NwGWucgtb|~>c&SAv96Ht^{3rgQ!)ZNgN?jHwngpSa0
zzaZAMI`gNInG|tdiITAd@bW4IFWat=>SQSoHi;B_?r$)9AHwjlujr!06;{mx?vuw0
zcBk_Asig`V6)jArGL8+^gDfxxHo8uM+lfzbB4yDvLo~qhS7|)<*<zN+^2Eyv&Sdq@
z2n}iv@O<nP?GBrS-XJ9`_&8$Y(+@-H)K93cD-Gw{6ucu_&IzVGG{9q65oM+b-GmtT
zksH&G#ZaDd62m)p^1#9_IvT$f8(L=YS>T(nepU-s`R`LNRgt%ghDW_MFs~Se{!S~*
zl0@OgK3kl&TwrJ@;2Jv_$A;BFbcqQpTRh=e_*!h*s^MtgGrVTd3T3l%WYu_*Pbth$
zc-2T4b`E6eax}&rW}AK$ef`EU{GhoVKK9#<GDSH^9bQ1d+I-H5-b1D4Z>gfco)YDa
zcs^9eFRSP1$PZhnOm7gpNXK`hJ3-}^i>BcFWRmJ3x8N62PaB}Dwn&(Mumo=hyhW;F
zBZ~1(T-G`U_1&ih<NC!|8F7Oib+my`1@LI~h;iq>5-j~boSVN45a(*T>HH>7N>rVN
zNUUJ_TpA30UJh5K?LuaEnQ>XlINeQ?kS0TgiK9)pX?l{H9~)@6?mIj_wi!bcr^UjT
z9yIT?nuUATxav*=tF`VpU`Be;7r<@{ztR^rQD|%X4JD+DAu|s4aNo}dJI4=lVw@+u
z`wGzhQ5AJu45Rb_1+B>6hMGVdjvl%V7h0=@$(}-XT>e^+vn^cfo(j1;+yg^mmS8C?
zLihLXfk(t?&W%YD>bxd|ho1gW?fDu!BZnbXE1~VFa@M8#VRODUrS31_C3fbJDuXp1
z)#qVkP794bDG&xnhCzpul#!7H&vxwM?D%v{@yWv4;8G~=bHdA*ibuznin9;%NVVg+
MXj!R62g`B%9~_|UKmY&$

literal 0
HcmV?d00001

diff --git a/samples/shell/reidentification/gallery/features/snapshot_PROF_GREG_J_ASHWORTH_4_frame_0_idx_0.tensor b/samples/shell/reidentification/gallery/features/snapshot_PROF_GREG_J_ASHWORTH_4_frame_0_idx_0.tensor
new file mode 100644
index 0000000000000000000000000000000000000000..409fe5e14b213bcf0e057a3455a9a233a0aa0afa
GIT binary patch
literal 1024
zcmWO4k6X-l902f>O3OWorbtacaP*vzliTrmzfV*cx$$HqqP49^@?&Ywcy>I8RCE#5
z^<#RFo}P1U6;G<~``r&(l&wOunw1|tHEXS>Cmt-dz5a$*W`+`agWkdT&M-C=Jwj9I
zzhq7c#O=Kj7+2kudVW3+_YY_BtG;};8?gaj&z<Cucp0L*63BaZBWoMosv{m4r3Gdi
z4t-kyN0!bb<(uQ^^H(3OX$j}EWfyRnp;Z`{7II(he935@hDJl4*yEXn>Rx+l?EjEw
z=d5Mz>I^s~9)VfAgT$t-7(6YT{r*S-)v0TwOq>Nj%(Iu?_^QP5L_z2J&SFtu7SHmF
zVfmp!u<Z6`+k6ekLhPzP3YDl+afbBUE65!64a(0y!^EC5*zwvA^NOG0J~s<m*@xkY
z6V6;1og$^m9I(NAGx<%~#rm)dLf0`{sO}EHcWdv^)82OSpP`^`PoL^auwnl@Iq<Y|
zI?7YLsJOC;ZfSq#{A0;n(vyZhZBCfAB>=zhwqn`V{_3aWWNe5S5(lsN(!$VOVcB5h
zsDfllHjM~NKqemeZ3n&5tI^KkbLxt%g~1i^Qp5gSPCfXO*qgG2hKI|jeA|57S$kU)
z-kPX=G2kd(`cGnEUyjBfS1&KuMREG`7?zv{;O(U<ELjxApH2A%XW<YPD+<Wp_9E9O
zIB-VL-=xp$ts0Hj^W|WG%-jO*@l40D@N9Y?senG~MEG=kC5&x8#82W(WPLY{OGcfc
zWz8ZH9oYo=-&$c&(HqF!qQ-y-9oi{tK<4I$#oFu8@tD|h)CIkBj2y2ta9V6UTP`kS
zAEzDcxY$j4J@_#m(>Zc><bCSyudmX?Dfs5s2l>V|3(HL{SoKvL-!v`d&e%ej(BaF&
z6&^TrJrr%#LEN@J4OJ&I$UN2zb{iE~Z=P7wUA3D0xgFzAuNSM{KL+P*Cvk>b9Tv*=
zlZ(d*`cmxSq-9pxtL{x;)(}Mv`LSm4D;VK@sMeJ6jU&tPq}N<rneb4!lM9y{N=0|~
zCT#3%mQ=A%AVlp7OXf`F_Nhnzf1O@^cz6XWt>*%_)<f#LbMR;IC^^hL4EOr4Nl|gr
z_>;EDJhC^Fb|$BQeBABoK`8?smOm%`uh|?{6NXoPO=NnshZ@gEz{Ulx*!<6RRE@tN
zrEdHH<;rbX{2+)g1p?<K+u#fL?cA_#GD52_8}br_Zn{RyQ6|C6Ni|egdQ{xoY=DOF
z4P1U;D>*v5a_lZQS`wH;-Ek9eWknaYcx8y!x(WQQN(mkcZ*ejFrQ}rP!RDb7%Gxv}
Z>dr8!zAxgy^bouf6T(fYHV{3E@E_T*@VWp1

literal 0
HcmV?d00001

diff --git a/samples/shell/reidentification/gallery/gallery.json b/samples/shell/reidentification/gallery/gallery.json
new file mode 100644
index 0000000..327618f
--- /dev/null
+++ b/samples/shell/reidentification/gallery/gallery.json
@@ -0,0 +1 @@
+{"Person-One_pink_hair_girl": {"features": ["features/Person-One_pink_hair_girl_7_frame_0_idx_0.tensor"]}, "Basketball_Player": {"features": ["features/Basketball_Player_0_frame_0_idx_0.tensor", "features/Basketball_Player_0_frame_0_idx_1.tensor"]}, "Person-One_yellow_hair_girl": {"features": ["features/Person-One_yellow_hair_girl_5_frame_0_idx_0.tensor"]}, "Person-Two_man": {"features": ["features/Person-Two_man_2_frame_0_idx_0.tensor"]}, "Person-One_curly_hair_boy": {"features": ["features/Person-One_curly_hair_boy_1_frame_0_idx_0.tensor"]}, "TomHanks": {"features": ["features/TomHanks_8_frame_0_idx_0.tensor"]}, "snapshot_COR_VAN_DER_KLAAUW": {"features": ["features/snapshot_COR_VAN_DER_KLAAUW_3_frame_0_idx_0.tensor"]}, "snapshot_PROF_GREG_J_ASHWORTH": {"features": ["features/snapshot_PROF_GREG_J_ASHWORTH_4_frame_0_idx_0.tensor"]}, "snapshot_DAVID_HEMBROW": {"features": ["features/snapshot_DAVID_HEMBROW_6_frame_0_idx_0.tensor"]}}
\ No newline at end of file
diff --git a/samples/shell/reidentification/gallery_generator.py b/samples/shell/reidentification/gallery_generator.py
new file mode 100644
index 0000000..6244aca
--- /dev/null
+++ b/samples/shell/reidentification/gallery_generator.py
@@ -0,0 +1,143 @@
+#!/bin/python3
+# ==============================================================================
+# Copyright (C) <2018-2019> Intel Corporation
+#
+# SPDX-License-Identifier: MIT
+# ==============================================================================
+
+import json
+import argparse
+import os
+import subprocess
+import re
+import fnmatch
+ 
+
+description = """
+Sample tool to generate feature database by image folder using gstreamer/ffmpeg pipeline.
+
+The name of the label is chosen as follows:
+1) filename - if image is in the root of image folder
+2) folder name - if image is in the subfolder
+"""
+
+def find_files(directory, pattern='*.*'):
+    if not os.path.exists(directory):
+        raise ValueError("Directory not found {}".format(directory))
+
+    matches = []
+    for root, _, filenames in os.walk(directory):
+        for filename in filenames:
+            full_path = os.path.join(root, filename)
+            if fnmatch.filter([full_path], pattern):
+                matches.append(os.path.join(root, filename))
+    return matches
+
+def get_models_path():
+    models_path = os.getenv("MODELS_PATH", None)
+
+    if models_path is None:
+        models_path = os.getenv("INTEL_CVSDK_DIR", None)
+        if models_path is not None:
+            models_path = os.path.join(models_path, "deployment_tools", "intel_models")
+            pass
+        pass
+    if models_path is None:
+        print("Warning: default models path not found by envs MODELS_PATH and INTEL_CVSDK_DIR")
+        pass
+    return models_path
+
+def find_model_path(model_name, models_dir_list):
+    model_path_list = []
+    for models_dir in models_dir_list:
+        if not os.path.exists(models_dir):
+            continue
+        for file_path in find_files(models_dir, "*.xml"):
+            if model_name.lower() in file_path.lower():
+                model_path_list.append(file_path)
+    return model_path_list
+
+def find_models_paths(model_names, models_dir_list, precision="FP32"):
+    if not model_names:
+        raise ValueError("Model names are not set")
+    if not models_dir_list:
+        raise ValueError("Model directories are not set")        
+ 
+    d = {}
+    for model_name in model_names:
+        d[model_name] = None
+        model_path_list = find_model_path(model_name, models_dir_list)
+        if not model_path_list:
+            continue
+        l = list(filter(lambda x: precision.lower() in x.lower(), model_path_list))
+        if not l:
+            print("Warning: can't find model: {} with precission: {}".format(model_name, precision))
+            continue    
+        d[model_name] = l.pop()
+    return d
+
+pipeline_template = "../../../ffmpeg -i {input_file} -vf \"detect=model={detection_model},classify=model={identification_model}, \
+        metaconvert=model={identification_model}:converter=tensors-to-file:method={label}:location={output_dir}\" -f null -"
+
+feature_file_regexp_template = r"^{label}_\d+_frame_\d+_idx_\d+.tensor$"
+
+
+default_detection_model = "face-detection-adas-0001"
+default_identification_model = "face-reidentification-retail-0095"
+default_models_paths = None if not get_models_path() else get_models_path().split(":")
+models_paths = find_models_paths([default_detection_model, default_identification_model], default_models_paths, "FP32")
+
+default_detection_path = models_paths.get(default_detection_model)
+default_identification_path = models_paths.get(default_identification_model)
+default_output = os.path.curdir
+
+
+def parse_arg():
+    parser = argparse.ArgumentParser(description=description, formatter_class=argparse.RawTextHelpFormatter)
+
+    parser.add_argument("--source_dir", "-s", required=True, help="Path to the folder with images")
+    parser.add_argument("--output", "-o", default=default_output, help="Path to output folder")
+    parser.add_argument("--detection", "-d", default=default_detection_path, help="Path to detection model xml file")
+    parser.add_argument("--identification", "-i", default=default_identification_path, help="Path to identification model xml file")
+
+    return parser.parse_args()
+
+
+if __name__ == "__main__":
+    args = parse_arg()
+
+    output_path = args.output
+    features_out = os.path.join(output_path, "features")
+    relative_features_out = os.path.relpath(features_out, output_path)
+    if not os.path.exists(features_out):
+        os.makedirs(features_out)
+
+    gallery = {}
+
+    for folder, subdir_list, file_list in os.walk(args.source_dir):
+        for idx, filename in enumerate(file_list):
+            label = os.path.splitext(filename)[0] if folder == args.source_dir else os.path.basename(folder)
+            abs_path = os.path.join(os.path.abspath(folder), filename)
+            pipeline = pipeline_template.format(input_file=abs_path, detection_model=args.detection,
+                                                identification_model=args.identification, label=(label + "_" + str(idx)), output_dir=features_out)
+            proc = subprocess.Popen(pipeline, shell=True)
+            if proc.wait() != 0:
+                print("Error while running pipeline")
+                exit(-1)
+
+            if label not in gallery:
+                gallery[label] = {'features': []}
+                pass
+            pass
+        pass
+
+    output_files = os.listdir(features_out)
+
+    for label in gallery.keys():
+        regexp = re.compile(feature_file_regexp_template.format(label=label))
+        gallery[label]['features'] = [os.path.join(relative_features_out, x) for x in output_files if regexp.match(x)]
+        pass
+    with open(os.path.join(output_path, "gallery.json"), 'w') as f:
+        json.dump(gallery, f)
+        pass
+    pass
-- 
2.7.4

